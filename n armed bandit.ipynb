{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading some required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising functions realted to the action values ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action values are draw from a Gaussian distribution with mean 0 and variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_action_values(n::Int64) = [randn() for i=1:n]\n",
    "\n",
    "#function draw_action_values(n::Int64)\n",
    "#    return [randn() for i=1:n]\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test draw_action_values function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q_star = draw_action_values(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action values are randomised by adding a value from a Gaussian distribution with mean 0 and variance 1 to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomise_action_value(value::Float64) = value + randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "test randomisation of action values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i = 1:10\n",
    "    println(randomise_action_value(Q_star[1]))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to evaluate exploration methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function evaluate_method(method::Function, nr_of_actions::Int64, nr_of_plays::Int64, nr_of_trials::Int64)\n",
    "    average_reward_per_action = zeros(nr_of_plays, nr_of_trials)\n",
    "    percentage_of_best_action = zeros(nr_of_plays, nr_of_trials)\n",
    "    for t = 1:nr_of_trials\n",
    "        # draw Q^*(a) and sort it, so that action 1 is always the best\n",
    "        Q_star = sort(draw_action_values(nr_of_actions), rev=true) # ground truth\n",
    "        Q_est  = zeros(nr_of_actions) # estimated action-values\n",
    "        Q_sum  = zeros(nr_of_actions) # sum of all rewards for each action\n",
    "        k_a    = zeros(nr_of_actions) # counts of times action a was chosen\n",
    "        for p = 1:nr_of_plays\n",
    "            selected_action = method(Q_est)\n",
    "            k_a[selected_action]   = k_a[selected_action] + 1.0\n",
    "            randomised_action_value = Q_star[selected_action] + randn()\n",
    "            Q_sum[selected_action] = Q_sum[selected_action] + randomised_action_value\n",
    "            Q_est[selected_action] = Q_sum[selected_action] / k_a[selected_action]\n",
    "            average_reward_per_action[p,t] = randomised_action_value\n",
    "            percentage_of_best_action[p,t] = (selected_action == 1)?1:0\n",
    "        end\n",
    "    end\n",
    "    return mean(average_reward_per_action,2), mean(percentage_of_best_action,2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy and ϵ-greedy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function ϵ_greedy(ϵ::Float64, Q::Vector{Float64})\n",
    "    action = nothing\n",
    "    if rand() >= ϵ || ϵ == 0.0\n",
    "        # get the highest estimated value\n",
    "        max_Q            = maximum(Q)\n",
    "        # get the list of actions with the highest value\n",
    "        possible_actions = findin(Q,max_Q)\n",
    "        # chose randomly from the list of actions with the highest values\n",
    "        action           = possible_actions[ceil(rand() * length(possible_actions))]\n",
    "    else\n",
    "        # chose randomly from all actions\n",
    "        action = ceil(rand() * length(Q))\n",
    "    end\n",
    "    return action\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define greedy als 0-greedy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "greedy(Q::Vector{Float64}) = ϵ_greedy(0.0, Q);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate greedy and ϵ-greedy functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_greedy,p_greedy=evaluate_method(greedy, 10, 1000, 2000);\n",
    "\n",
    "ϵ_greedy_with_001(Q::Vector{Float64})=ϵ_greedy(0.01, Q)\n",
    "r_ϵ_001,p_ϵ_001=evaluate_method(ϵ_greedy_with_001, 10, 1000, 2000);\n",
    "\n",
    "ϵ_greedy_with_01(Q::Vector{Float64})=ϵ_greedy(0.1, Q)\n",
    "r_ϵ_01,p_ϵ_01=evaluate_method(ϵ_greedy_with_01, 10, 1000, 2000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the average reward per action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg    = plot(r_greedy)\n",
    "p_001 = plot(r_ϵ_001)\n",
    "p_01  = plot(r_ϵ_01)\n",
    "legend( [pg,p_001,p_01], [\"greedy\", \"e = 0.01\", \"e = 0.1\"], loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting how often the correct actions was chosen on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg    = plot(p_greedy)\n",
    "p_001 = plot(p_ϵ_001)\n",
    "p_01  = plot(p_ϵ_01)\n",
    "legend( [pg,p_001,p_01], [\"greedy\", \"eps = 0.01\", \"eps = 0.1\"], loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function softmax(tau::Float64, Q::Vector{Float64})\n",
    "    probabilities = [exp(Q[a] / tau) for a=1:length(Q)]\n",
    "    s             = sum(probabilities)\n",
    "    probabilities = probabilities ./ s\n",
    "    p  = rand()\n",
    "    pa = 0\n",
    "    for a = 1:length(Q)\n",
    "        pa = pa + probabilities[a]\n",
    "        if p < pa\n",
    "            return a\n",
    "        end\n",
    "    end\n",
    "    return length(Q)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "plots = []\n",
    "\n",
    "for t in [0:0.25:2]\n",
    "    softmax_func(Q::Vector{Float64}) = softmax(t, Q)\n",
    "    a_softmax,p_softmax=evaluate_method(softmax_func, 10, 1000, 2000)\n",
    "    p = plot(p_softmax)\n",
    "    plots = [plots, p]\n",
    "    l = [l, \"tau=$t\"]\n",
    "end\n",
    "\n",
    "legend(plots, l, loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimistic initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function evaluate_optimistic_method(method::Function, nr_of_actions::Int64, nr_of_plays::Int64, nr_of_trials::Int64, Q_init::Float64)\n",
    "    average_reward_per_action = zeros(nr_of_plays, nr_of_trials)\n",
    "    percentage_of_best_action = zeros(nr_of_plays, nr_of_trials)\n",
    "    for t = 1:nr_of_trials\n",
    "        # draw Q^*(a) and sort it, so that action 1 is always the best\n",
    "        Q_star = sort(draw_action_values(nr_of_actions), rev=true) # ground truth\n",
    "        Q_est  = ones(nr_of_actions) .* Q_init # estimated action-values\n",
    "        Q_sum  = zeros(nr_of_actions) # sum of all rewards for each action\n",
    "        k_a    = zeros(nr_of_actions) # counts of times action a was chosen\n",
    "        for p = 1:nr_of_plays\n",
    "            selected_action = method(Q_est)\n",
    "            k_a[selected_action]   = k_a[selected_action] + 1.0\n",
    "            randomised_action_value = Q_star[selected_action] + randn()\n",
    "            Q_sum[selected_action] = Q_sum[selected_action] + randomised_action_value\n",
    "            Q_est[selected_action] = Q_sum[selected_action] / k_a[selected_action]\n",
    "            average_reward_per_action[p,t] = randomised_action_value\n",
    "            percentage_of_best_action[p,t] = (selected_action == 1)?1:0\n",
    "        end\n",
    "    end\n",
    "    return mean(average_reward_per_action,2), mean(percentage_of_best_action,2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_greedy_optimisitic,p_greedy_optimisitic=evaluate_optimistic_method(greedy, 10, 1000, 2000, 5.0);\n",
    "\n",
    "ϵ_greedy_with_001(Q::Vector{Float64})=ϵ_greedy(0.01, Q)\n",
    "r_ϵ_001_optimisitic,p_ϵ_001_optimisitic=evaluate_optimistic_method(ϵ_greedy_with_001, 10, 1000, 2000, 5.0);\n",
    "\n",
    "ϵ_greedy_with_01(Q::Vector{Float64})=ϵ_greedy(0.1, Q)\n",
    "r_ϵ_01_optimisitic,p_ϵ_01_optimisitic=evaluate_optimistic_method(ϵ_greedy_with_01, 10, 1000, 2000, 5.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg    = plot(r_greedy)\n",
    "p_001 = plot(r_ϵ_001)\n",
    "p_01  = plot(r_ϵ_01)\n",
    "\n",
    "pg_opt    = plot(r_greedy_optimisitic)\n",
    "p_001_opt = plot(r_ϵ_001_optimisitic)\n",
    "p_01_opt  = plot(r_ϵ_01_optimisitic)\n",
    "\n",
    "legend( [pg, p_001, p_01, pg_opt, p_001_opt, p_01_opt], [\"greedy\", \"e = 0.01\", \"e = 0.1\",\"greedy (optimisitic)\", \"e = 0.01 (optimisitic)\", \"e = 0.1 (optimisitic)\"], loc=4)\n",
    "title(\"Average reward per action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg    = plot(r_greedy[1:100])\n",
    "p_001 = plot(r_ϵ_001[1:100])\n",
    "p_01  = plot(r_ϵ_01[1:100])\n",
    "\n",
    "pg_opt    = plot(r_greedy_optimisitic[1:100])\n",
    "p_001_opt = plot(r_ϵ_001_optimisitic[1:100])\n",
    "p_01_opt  = plot(r_ϵ_01_optimisitic[1:100])\n",
    "\n",
    "legend( [pg, p_001, p_01, pg_opt, p_001_opt, p_01_opt], [\"greedy\", \"e = 0.01\", \"e = 0.1\",\"greedy (optimisitic)\", \"e = 0.01 (optimisitic)\", \"e = 0.1 (optimisitic)\"], loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg    = plot(p_greedy)\n",
    "p_001 = plot(p_ϵ_001)\n",
    "p_01  = plot(p_ϵ_01)\n",
    "\n",
    "pg_opt    = plot(p_greedy_optimisitic)\n",
    "p_001_opt = plot(p_ϵ_001_optimisitic)\n",
    "p_01_opt  = plot(p_ϵ_01_optimisitic)\n",
    "\n",
    "legend( [pg, p_001, p_01, pg_opt, p_001_opt, p_01_opt], [\"greedy\", \"e = 0.01\", \"e = 0.1\",\"greedy (optimisitic)\", \"e = 0.01 (optimisitic)\", \"e = 0.1 (optimisitic)\"], loc=4)\n",
    "title(\"Percentage of correct actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg    = plot(p_greedy[1:100])\n",
    "p_001 = plot(p_ϵ_001[1:100])\n",
    "p_01  = plot(p_ϵ_01[1:100])\n",
    "\n",
    "pg_opt    = plot(p_greedy_optimisitic[1:100])\n",
    "p_001_opt = plot(p_ϵ_001_optimisitic[1:100])\n",
    "p_01_opt  = plot(p_ϵ_01_optimisitic[1:100])\n",
    "\n",
    "legend( [pg, p_001, p_01, pg_opt, p_001_opt, p_01_opt], [\"greedy\", \"e = 0.01\", \"e = 0.1\",\"greedy (optimisitic)\", \"e = 0.01 (optimisitic)\", \"e = 0.1 (optimisitic)\"], loc=4);\n",
    "title(\"Percentage of correct actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function reinforcement_comparision(p::Vector{Float64})\n",
    "    probabilities = [exp(p[a]) for a=1:length(p)]\n",
    "    s             = sum(probabilities)\n",
    "    probabilities = probabilities ./ s\n",
    "    pr  = rand()\n",
    "    pa = 0\n",
    "    for a = 1:length(pr)\n",
    "        pa = pa + probabilities[a]\n",
    "        if pr < pa\n",
    "            return a\n",
    "        end\n",
    "    end\n",
    "    return length(p)\n",
    "end\n",
    "\n",
    "function evaluate_reinforcement_comparision(nr_of_actions::Int64, nr_of_plays::Int64, nr_of_trials::Int64, α::Float64, β::Float64)\n",
    "    average_reward_per_action = zeros(nr_of_plays, nr_of_trials)\n",
    "    percentage_of_best_action = zeros(nr_of_plays, nr_of_trials)\n",
    "    for t = 1:nr_of_trials\n",
    "        # draw Q^*(a) and sort it, so that action 1 is always the best\n",
    "        Q_star   = sort(draw_action_values(nr_of_actions), rev=true) # ground truth\n",
    "        Q_est    = zeros(nr_of_actions) # estimated action-values\n",
    "        Q_sum    = zeros(nr_of_actions) # sum of all rewards for each action\n",
    "        k_a      = zeros(nr_of_actions) # counts of times action a was chosen\n",
    "        pa       = zeros(nr_of_actions)\n",
    "        baseline = 0\n",
    "        for p = 1:nr_of_plays\n",
    "            selected_action                = reinforcement_comparision(pa)\n",
    "            k_a[selected_action]           = k_a[selected_action] + 1.0\n",
    "            randomised_action_value        = Q_star[selected_action] + randn()\n",
    "            pa[selected_action]            = pa[selected_action] + β * (randomised_action_value - baseline)\n",
    "            baseline                       = baseline + α * (randomised_action_value - baseline)\n",
    "            Q_sum[selected_action]         = Q_sum[selected_action] + randomised_action_value\n",
    "            Q_est[selected_action]         = Q_sum[selected_action] / k_a[selected_action]\n",
    "            average_reward_per_action[p,t] = randomised_action_value\n",
    "            percentage_of_best_action[p,t] = (selected_action == 1)?1:0\n",
    "        end\n",
    "    end\n",
    "    return mean(average_reward_per_action,2), mean(percentage_of_best_action,2)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r,p = evaluate_reinforcement_comparision(10, 1000, 2000, 0.1, 0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg    = plot(r_greedy)\n",
    "p_001 = plot(r_ϵ_001)\n",
    "p_01  = plot(r_ϵ_01)\n",
    "\n",
    "pg_opt    = plot(r_greedy_optimisitic)\n",
    "p_001_opt = plot(r_ϵ_001_optimisitic)\n",
    "p_01_opt  = plot(r_ϵ_01_optimisitic)\n",
    "\n",
    "prc = plot(r)\n",
    "\n",
    "legend( [pg, p_001, p_01, pg_opt, p_001_opt, p_01_opt, prc], [\"greedy\", \"e = 0.01\", \"e = 0.1\",\"greedy (optimisitic)\", \"e = 0.01 (optimisitic)\", \"e = 0.1 (optimisitic)\", \"reinforcement comparison\"], loc=4)\n",
    "title(\"Average reward per action\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg    = plot(p_greedy)\n",
    "p_001 = plot(p_ϵ_001)\n",
    "p_01  = plot(p_ϵ_01)\n",
    "\n",
    "pg_opt    = plot(p_greedy_optimisitic)\n",
    "p_001_opt = plot(p_ϵ_001_optimisitic)\n",
    "p_01_opt  = plot(p_ϵ_01_optimisitic)\n",
    "\n",
    "prc = plot(p)\n",
    "\n",
    "legend( [pg, p_001, p_01, pg_opt, p_001_opt, p_01_opt, prc], [\"greedy\", \"e = 0.01\", \"e = 0.1\",\"greedy (optimisitic)\", \"e = 0.01 (optimisitic)\", \"e = 0.1 (optimisitic)\", \"reinforcement comparison\"], loc=4);\n",
    "title(\"Percentage of correct actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg    = plot(p_greedy[1:200])\n",
    "p_001 = plot(p_ϵ_001[1:200])\n",
    "p_01  = plot(p_ϵ_01[1:200])\n",
    "\n",
    "pg_opt    = plot(p_greedy_optimisitic[1:200])\n",
    "p_001_opt = plot(p_ϵ_001_optimisitic[1:200])\n",
    "p_01_opt  = plot(p_ϵ_01_optimisitic[1:200])\n",
    "\n",
    "prc = plot(p[1:200])\n",
    "\n",
    "legend( [pg, p_001, p_01, pg_opt, p_001_opt, p_01_opt, prc], [\"greedy\", \"e = 0.01\", \"e = 0.1\",\"greedy (optimisitic)\", \"e = 0.01 (optimisitic)\", \"e = 0.1 (optimisitic)\", \"reinforcement comparison\"], loc=4);\n",
    "title(\"Percentage of correct actions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.6",
   "language": "julia",
   "name": "julia 0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
